{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AAsN4JIWsZw",
        "outputId": "76055587-eb70-4895-fc8c-ba5015b8fc26"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://gitlab-research.centralesupelec.fr/stergios.christodoulidis/text-flappy-bird-gym.git\n",
        "#code was run in google collab as text flappy conflict with existing gym version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YpJCUbx3UEAd"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import gymnasium as gym\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import text_flappy_bird_gym\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_1fabAcZjn-"
      },
      "outputs": [],
      "source": [
        "env=gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)\n",
        "obs = env.reset()\n",
        "seed=420\n",
        "np.random.seed(seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdTM4aTmtIxw",
        "outputId": "bb6b5f8c-0075-4ce7-c485-e55d2004ea49"
      },
      "outputs": [],
      "source": [
        "temp=(0,0)\n",
        "\n",
        "#this code showcase that for some reasons, env.step can return something outside of the observation spaces, which is strange and prevents dictionnary initializations.\n",
        "count=0\n",
        "\n",
        "while env.observation_space.contains(temp) and count<1000:\n",
        "  action=env.action_space.sample()\n",
        "  temp,reward,done,_,info=env.step(action)\n",
        "  print(temp)\n",
        "  count+=1\n",
        "print('count',count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrihklXlfZPT",
        "outputId": "cb4e6231-c55d-4b89-8ece-110ecc4cba65"
      },
      "outputs": [],
      "source": [
        "print(env.observation_space,env.action_space) #from that we derive the dimensions (can be adapted make env dependant )\n",
        "\n",
        "state_dim=(14,22)\n",
        "state_off=11\n",
        "action_dim=2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2--6BPJ6ZHk9"
      },
      "source": [
        "Below the $\\lambda$ SARSA agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9yhRCWaW9NU"
      },
      "outputs": [],
      "source": [
        "class LambdaSarsaAgent:\n",
        "    def __init__(self,state_dim, state_off, actions_dim, alpha=0.02, gamma=0.99, epsilon=0.1, lambda_val=0.1):\n",
        "        self.actions_dim = actions_dim\n",
        "        self.state_dim=state_dim\n",
        "        self.offset=state_off-1\n",
        "        self.q_table = np.random.uniform(low=-1,high=1,size=(state_dim[0],state_dim[1],action_dim))\n",
        "        self.e_trace = np.zeros((state_dim[0],state_dim[1],action_dim))\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate for epsilon-greedy policy\n",
        "        self.lambda_val = lambda_val  # Eligibility trace decay\n",
        "\n",
        "\n",
        "    def get_epsilon_greedy_action(self, state):\n",
        "        \"\"\"Select an action using an epsilon-greedy policy.\"\"\"\n",
        "        best_action = np.argmax(self.q_table[state[0], state[1] + self.offset])\n",
        "\n",
        "        if np.random.uniform(0, 1) < self.epsilon:\n",
        "            return np.random.choice(self.actions_dim)  # Random action (explore)\n",
        "        return best_action  # Greedy action (exploit)\n",
        "\n",
        "    def get_greedy_action(self, state):\n",
        "        \"\"\"Select the best action (greedy policy, no exploration).\"\"\"\n",
        "        return np.argmax(self.q_table[state[0], state[1] + self.offset])\n",
        "\n",
        "    def update(self, state, action, reward, next_state, next_action, done):\n",
        "\n",
        "        delta=reward+(0 if done else self.gamma*self.q_table[next_state[0],next_state[1]+self.offset,next_action]-self.q_table[state[0],state[1]+self.offset,action])\n",
        "        self.e_trace[state[0],state[1]+self.offset,action]+=1# replacing the trace\n",
        "\n",
        "        self.q_table+=self.alpha*delta*self.e_trace\n",
        "        self.e_trace*=self.gamma*self.lambda_val\n",
        "\n",
        "    def train(self,env,episodes=20000,verbose=False):\n",
        "      episode_reward=[]\n",
        "      for episode in range(episodes):\n",
        "          if verbose and episode %1000==0:\n",
        "            print(episode)\n",
        "          state = env.reset()[0]\n",
        "          action = self.get_epsilon_greedy_action(state) #we train w/ epsilon greedy policy\n",
        "          done = False\n",
        "          total_reward = 0\n",
        "\n",
        "\n",
        "          while not done:\n",
        "              next_state, reward, done, _,info = env.step(action)\n",
        "              next_action =self.get_epsilon_greedy_action(next_state) if not done else None\n",
        "              self.update(state, action, reward, next_state, next_action, done)\n",
        "              state, action = next_state, next_action\n",
        "              total_reward += reward\n",
        "\n",
        "          episode_reward.append(total_reward)\n",
        "\n",
        "          #print(f\"Episode {episode+1}: Total Reward = {total_reward}\")\n",
        "      return episode_reward\n",
        "\n",
        "\n",
        "    def reset_traces(self):\n",
        "        self.e_trace *=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEdxF2pQxfbH"
      },
      "outputs": [],
      "source": [
        "class MonteCarloAgent:\n",
        "    def __init__(self, state_space,state_offset, action_dim, alpha=0.01,gamma=0.99, epsilon=0.1):\n",
        "        self.state_space = state_space\n",
        "        self.offset=state_offset-1\n",
        "        self.action_space = action_dim\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration probability\n",
        "        self.alpha=alpha\n",
        "\n",
        "        # Initialize Q-table and Returns storage\n",
        "        self.q_table = np.random.uniform(low=-1,high=1,size=(self.state_space[0], self.state_space[1], self.action_space))\n",
        "\n",
        "\n",
        "    def get_epsilon_greedy_action(self, state):\n",
        "        \"\"\"Select an action using an epsilon-greedy policy.\"\"\"\n",
        "        best_action = np.argmax(self.q_table[state[0], state[1] + self.offset])\n",
        "\n",
        "        if np.random.uniform(0, 1) < self.epsilon:\n",
        "            return np.random.choice(self.action_space)  # Random action (explore)\n",
        "        return best_action  # Greedy action (exploit)\n",
        "\n",
        "    def get_greedy_action(self, state):\n",
        "        \"\"\"Select the best action (greedy policy, no exploration).\"\"\"\n",
        "        return np.argmax(self.q_table[state[0], state[1] + self.offset])\n",
        "\n",
        "\n",
        "    def update(self, episode_data):\n",
        "        \"\"\"Monte Carlo update after an episode\"\"\"\n",
        "        G = 0  # Initialize return\n",
        "        visited = set()\n",
        "\n",
        "        for t in reversed(range(len(episode_data))):\n",
        "            state, action, reward = episode_data[t]\n",
        "            G = reward + self.gamma * G  # Discounted return\n",
        "\n",
        "            if (state, action) not in visited:\n",
        "                visited.add((state, action))\n",
        "                self.q_table[state[0],state[1]+self.offset, action] += self.alpha * (G - self.q_table[state[0],state[1]+self.offset, action])\n",
        "\n",
        "    def train(self,env, episodes=20000,verbose=False):\n",
        "        \"\"\"Train agent using Monte Carlo Control\"\"\"\n",
        "        rewards=[]\n",
        "        for episode in range(episodes):\n",
        "\n",
        "            if verbose and episode %1000==0:\n",
        "              print(episode)\n",
        "            state =env.reset()[0]  # Reset environment\n",
        "            episode_data = []\n",
        "            total_reward=0\n",
        "\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.get_epsilon_greedy_action(state)\n",
        "\n",
        "                next_state,reward,done,_,info=env.step(action)\n",
        "\n",
        "\n",
        "                episode_data.append((state, action, reward))\n",
        "                total_reward+=reward\n",
        "                state = next_state\n",
        "\n",
        "            self.update(episode_data)  # Update Q-values\n",
        "            rewards.append(total_reward)\n",
        "\n",
        "        print(\"Training complete!\")\n",
        "        return rewards\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UtG_01RegzmY",
        "outputId": "8b003149-86a4-48c4-94a1-3db9b2fadd11"
      },
      "outputs": [],
      "source": [
        "Sarsa=LambdaSarsaAgent(state_dim,state_off,action_dim)\n",
        "\n",
        "sarsa_rewards=Sarsa.train(env,verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmkN30GPs6VA"
      },
      "outputs": [],
      "source": [
        "def exponential_moving_average(data, alpha=0.05):\n",
        "    ema = [data[0]]\n",
        "    for i in range(1, len(data)):\n",
        "        ema.append(alpha * data[i] + (1 - alpha) * ema[-1])\n",
        "    return np.array(ema)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "7uu3xMVysISY",
        "outputId": "2341eee9-2e65-4514-dbb4-0371caf2b4f2"
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage import uniform_filter1d #different way to smooth out\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "plt.plot(exponential_moving_average(sarsa_rewards)[::100]) #EMA Visualy looks the best, plot every100 point so that it doesnt looks as cluttered\n",
        "legend_text = f\"Max: {np.max(sarsa_rewards)}\\nMean: {np.mean(sarsa_rewards):.2f}\\nStd: {np.std(sarsa_rewards):.2f}\"\n",
        "plt.legend([legend_text], loc='lower right', fontsize=10, frameon=True)\n",
        "plt.title(\"Smoothed Sarsa reward over training over training\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Length of Episode\")\n",
        "plt.savefig('SARSAtrainig.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "D80Pgx1Cvp46",
        "outputId": "8b1c2534-e251-4049-9e15-16b701679054"
      },
      "outputs": [],
      "source": [
        "mcagent=MonteCarloAgent(state_space=state_dim,state_offset=state_off,action_dim=action_dim)\n",
        "mc_rewards=mcagent.train(env,verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "-x2HRzFX_fp1",
        "outputId": "4c853c67-47f1-40f5-cdbb-5041184f17dc"
      },
      "outputs": [],
      "source": [
        "plt.plot(exponential_moving_average(mc_rewards)[::100]) #EMA Visualy looks the best\n",
        "legend_text = f\"Max: {np.max(mc_rewards)}\\nMean: {np.mean(mc_rewards):.2f}\\nStd: {np.std(mc_rewards):.2f}\"\n",
        "plt.legend([legend_text], loc='lower right', fontsize=10, frameon=True)\n",
        "plt.title(\"Smoothed  MC reward over training over training\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Length of Episode\")\n",
        "plt.savefig('MCtrainig.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTrph-P_UpbL"
      },
      "source": [
        "Play function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPaXL3gvOClo"
      },
      "outputs": [],
      "source": [
        "def play(agent, env_param=(15,20,4), runs=10,verbose=False): #(15,20,4) is the env param on which both agents are trained\n",
        "    \"\"\"Plays one episode using a greedy policy and averages rewards over multiple runs.\"\"\"\n",
        "    total_rewards = []\n",
        "    h,w,pg=env_param\n",
        "    env=gym.make('TextFlappyBird-v0', height = h, width = w, pipe_gap = pg)\n",
        "    print(env.observation_space)\n",
        "    for i in range(runs):\n",
        "        if verbose:\n",
        "          print(i)\n",
        "\n",
        "        state = env.reset()[0] # Reset environment\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        count=0 #maximum play time\n",
        "\n",
        "        while not done and count<2000:\n",
        "            action = agent.get_greedy_action(state)  # we play with greedy policy\n",
        "            next_state, reward, done, _,info = env.step(action)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            count+=1\n",
        "\n",
        "        total_rewards.append(episode_reward)  # Store reward for this run\n",
        "\n",
        "    avg_reward = np.mean(total_rewards)  # Compute average reward over runs\n",
        "    print(f\"Average Reward over {runs} runs: {avg_reward:.2f}\")\n",
        "\n",
        "    return avg_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlXESYN06lx2"
      },
      "source": [
        "Here we test how does the agent perform on different env parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d01iqjdZQIxh",
        "outputId": "a9531847-b09e-4f80-a185-52c27a07d99e"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "sarsa_play_logs={}\n",
        "hs=[15] #changing the env changes the observation spaces param, so handpicked so that it doesn't creates a to big space\n",
        "ws=[15,20]\n",
        "pgs=[2,4,6,8]\n",
        "for (h,w,pg)in itertools.product(hs,ws,pgs):\n",
        "    avg_reward=play(Sarsa,(h,w,pg))\n",
        "    sarsa_play_logs[(h,w,pg)]=avg_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq5c1L7gUCtz",
        "outputId": "cb379ebb-dc7a-47e7-8419-bfc751471288"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "mc_play_logs={}\n",
        "hs=[15]\n",
        "ws=[15,20]\n",
        "pgs=[2,4,6,8]\n",
        "for (h,w,pg)in itertools.product(hs,ws,pgs):\n",
        "    avg_reward=play(mcagent,(h,w,pg))\n",
        "    sarsa_play_logs[(h,w,pg)]=avg_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jd0WClpVqkJ"
      },
      "source": [
        "We plot the Q values heatmaps for both agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CrRl31UpVtnu"
      },
      "outputs": [],
      "source": [
        "\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_side_by_side_heatmaps(array, name,offset=0):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    for i in range(2):\n",
        "        sns.heatmap(array[:, :, i].T, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5, ax=axes[i])\n",
        "        axes[i].set_title(f\"Q_values heatmap for actions {i}\")\n",
        "        axes[i].set_xlabel(\"x\")\n",
        "        axes[i].set_ylabel(\"y\")\n",
        "        axes[i].set_yticklabels(np.arange(array.shape[1]) - offset)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{name} agent Q value heatmap on default env.png',dpi=300)\n",
        "    plt.show()\n",
        "     #save fig to local dir\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "tdRcJBS9YAro",
        "outputId": "9d3abbcd-ecb5-48fa-bbfd-f15f76109d77"
      },
      "outputs": [],
      "source": [
        "plot_side_by_side_heatmaps(Sarsa.q_table,'sarsa',offset=Sarsa.offset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "1O4V01fBYjqV",
        "outputId": "b1fca286-e64e-4918-b3e4-8de76b0bd04b"
      },
      "outputs": [],
      "source": [
        "plot_side_by_side_heatmaps(mcagent.q_table,'mcagent',offset=mcagent.offset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLzrzFuqUsm8"
      },
      "source": [
        "Now we conduct parameters sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq5LdJ5NUZMb",
        "outputId": "098f5720-5381-454b-da09-7accf370336c"
      },
      "outputs": [],
      "source": [
        "env=gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)\n",
        "print(env.observation_space,env.action_space) #from that we derive the dimensions (can be adapted make env dependant )\n",
        "\n",
        "state_dim=(14,22)\n",
        "state_off=11\n",
        "action_dim=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_BlM17Mvcz0L"
      },
      "outputs": [],
      "source": [
        "def plot_heatmap(results, xticks, yticks,xlabel,ylabel,title):\n",
        "    \"\"\"\n",
        "    Plots a heatmap of agent performance with lambda and alpha varying.\n",
        "\n",
        "    :param results: A 2D NumPy array where results[i, j] corresponds to the performance metric\n",
        "                    for lambda=lambdas[i] and alpha=alphas[j].\n",
        "    :param lambdas: List of lambda values (Y-axis).\n",
        "    :param alphas: List of alpha values (X-axis).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(results, xticklabels=xticks, yticklabels=yticks, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.savefig(f'{title}.png',dpi=300)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBDp2UH6gTpp"
      },
      "source": [
        "Parameters sweep for Sarsa agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B6PBXVDbf3uG",
        "outputId": "b609faf5-4265-4bef-87a2-08485282e220"
      },
      "outputs": [],
      "source": [
        "\n",
        "lambdas=[0.1,0.3,0.5]\n",
        "alphas=[0.01,0.5,0.1]\n",
        "\n",
        "\n",
        "res=np.zeros((len(lambdas),len(alphas)))\n",
        "for i in range(len(lambdas)):\n",
        "  for j in range(len(alphas)):\n",
        "    print(i,j)\n",
        "    lbd,alpha=lambdas[i],alphas[j]\n",
        "    sarsaagent=LambdaSarsaAgent(state_dim,state_off,action_dim,alpha=alpha,lambda_val=lbd)\n",
        "    sarsaagent.train(env)\n",
        "    playrewards=play(sarsaagent)\n",
        "    res[i,j]=playrewards\n",
        "\n",
        "plot_heatmap(res,lambdas,alphas,'lambdas','alphas',' Sarsa Average greedy play time alpha')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4WSmG5_PCD_y",
        "outputId": "26e2c371-32a7-4483-cc1c-cfabc387ce2a"
      },
      "outputs": [],
      "source": [
        "\n",
        "lambdas=[0.1,0.3,0.5]\n",
        "epsilons=[0.05,0.1,0.3]\n",
        "\n",
        "\n",
        "res=np.zeros((len(lambdas),len(epsilons)))\n",
        "for i in range(len(lambdas)):\n",
        "  for j in range(len(epsilons)):\n",
        "    print(i,j)\n",
        "    lbd,alpha=lambdas[i],epsilons[j]\n",
        "    sarsaagent=LambdaSarsaAgent(state_dim,state_off,action_dim,alpha=alpha,lambda_val=lbd)\n",
        "    sarsaagent.train(env)\n",
        "    playrewards=play(sarsaagent)\n",
        "    res[i,j]=playrewards\n",
        "\n",
        "plot_heatmap(res,lambdas,alphas,'lambdas','epsilons',' Sarsa Average greedy play time epsilon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s4yQDzHwDaT0",
        "outputId": "4f1d2667-5271-4505-b819-ae6ae1541070"
      },
      "outputs": [],
      "source": [
        "\n",
        "alphas=[0.01,0.1,0.3,0.5]\n",
        "epsilons=[0.05,0.1,0.2,0.3]\n",
        "\n",
        "\n",
        "res=np.zeros((len(alphas),len(epsilons)))\n",
        "for i in range(len(alphas)):\n",
        "  for j in range(len(epsilons)):\n",
        "    print(i,j)\n",
        "    alpha,epsilon=alphas[i],epsilons[j]\n",
        "    mcagent=MonteCarloAgent(state_space=state_dim,state_offset=state_off,action_dim=action_dim,alpha=alpha,epsilon=epsilon)\n",
        "    mcagent.train(env)\n",
        "    playrewards=play(mcagent)\n",
        "    res[i,j]=playrewards\n",
        "\n",
        "plot_heatmap(res,alphas,epsilons,'alphas','epsilons',' MC Average greedy play time')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
